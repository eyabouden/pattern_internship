{
  "nbformat": 4,
  "nbformat_minor": 5,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "7ZH2qkXXv2ai"
      },
      "source": [
        "# GraphRAG from JSON graph\n"
      ],
      "id": "7ZH2qkXXv2ai"
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "H9K7tgTtv2ak"
      },
      "source": [
        "## 1) Install deps"
      ],
      "id": "H9K7tgTtv2ak"
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "gmD58R6uv2ak"
      },
      "execution_count": null,
      "outputs": [],
      "source": [
        "\n",
        "%%capture\n",
        "!pip -q install --upgrade pip\n",
        "!pip -q install langchain-core langchain-community langchain-chroma chromadb sentence-transformers pydantic==2.*\n",
        "!pip -q install networkx pyvis\n"
      ],
      "id": "gmD58R6uv2ak"
    },
    {
      "cell_type": "code",
      "source": [
        "# Fix Google packages to a compatible set\n",
        "!pip -q install --upgrade --no-cache-dir \\\n",
        "  \"google-generativeai==0.8.5\" \\\n",
        "  \"google-ai-generativelanguage==0.6.15\" \\\n",
        "  \"langchain-google-genai>=1.0.6\" \\\n",
        "  faiss-cpu\n"
      ],
      "metadata": {
        "id": "UZCWbt7-i2ao"
      },
      "id": "UZCWbt7-i2ao",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "VJL-COhOv2al"
      },
      "source": [
        "## 2) Upload your `case_studies_graph.json`"
      ],
      "id": "VJL-COhOv2al"
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 89
        },
        "id": "ulVbBiZxv2am",
        "outputId": "4f8e6817-6a76-46af-8978-8c6f0b1eeaa1"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              "\n",
              "     <input type=\"file\" id=\"files-615302e3-905e-4704-bcb7-027cee6c4f0a\" name=\"files[]\" multiple disabled\n",
              "        style=\"border:none\" />\n",
              "     <output id=\"result-615302e3-905e-4704-bcb7-027cee6c4f0a\">\n",
              "      Upload widget is only available when the cell has been executed in the\n",
              "      current browser session. Please rerun this cell to enable.\n",
              "      </output>\n",
              "      <script>// Copyright 2017 Google LLC\n",
              "//\n",
              "// Licensed under the Apache License, Version 2.0 (the \"License\");\n",
              "// you may not use this file except in compliance with the License.\n",
              "// You may obtain a copy of the License at\n",
              "//\n",
              "//      http://www.apache.org/licenses/LICENSE-2.0\n",
              "//\n",
              "// Unless required by applicable law or agreed to in writing, software\n",
              "// distributed under the License is distributed on an \"AS IS\" BASIS,\n",
              "// WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n",
              "// See the License for the specific language governing permissions and\n",
              "// limitations under the License.\n",
              "\n",
              "/**\n",
              " * @fileoverview Helpers for google.colab Python module.\n",
              " */\n",
              "(function(scope) {\n",
              "function span(text, styleAttributes = {}) {\n",
              "  const element = document.createElement('span');\n",
              "  element.textContent = text;\n",
              "  for (const key of Object.keys(styleAttributes)) {\n",
              "    element.style[key] = styleAttributes[key];\n",
              "  }\n",
              "  return element;\n",
              "}\n",
              "\n",
              "// Max number of bytes which will be uploaded at a time.\n",
              "const MAX_PAYLOAD_SIZE = 100 * 1024;\n",
              "\n",
              "function _uploadFiles(inputId, outputId) {\n",
              "  const steps = uploadFilesStep(inputId, outputId);\n",
              "  const outputElement = document.getElementById(outputId);\n",
              "  // Cache steps on the outputElement to make it available for the next call\n",
              "  // to uploadFilesContinue from Python.\n",
              "  outputElement.steps = steps;\n",
              "\n",
              "  return _uploadFilesContinue(outputId);\n",
              "}\n",
              "\n",
              "// This is roughly an async generator (not supported in the browser yet),\n",
              "// where there are multiple asynchronous steps and the Python side is going\n",
              "// to poll for completion of each step.\n",
              "// This uses a Promise to block the python side on completion of each step,\n",
              "// then passes the result of the previous step as the input to the next step.\n",
              "function _uploadFilesContinue(outputId) {\n",
              "  const outputElement = document.getElementById(outputId);\n",
              "  const steps = outputElement.steps;\n",
              "\n",
              "  const next = steps.next(outputElement.lastPromiseValue);\n",
              "  return Promise.resolve(next.value.promise).then((value) => {\n",
              "    // Cache the last promise value to make it available to the next\n",
              "    // step of the generator.\n",
              "    outputElement.lastPromiseValue = value;\n",
              "    return next.value.response;\n",
              "  });\n",
              "}\n",
              "\n",
              "/**\n",
              " * Generator function which is called between each async step of the upload\n",
              " * process.\n",
              " * @param {string} inputId Element ID of the input file picker element.\n",
              " * @param {string} outputId Element ID of the output display.\n",
              " * @return {!Iterable<!Object>} Iterable of next steps.\n",
              " */\n",
              "function* uploadFilesStep(inputId, outputId) {\n",
              "  const inputElement = document.getElementById(inputId);\n",
              "  inputElement.disabled = false;\n",
              "\n",
              "  const outputElement = document.getElementById(outputId);\n",
              "  outputElement.innerHTML = '';\n",
              "\n",
              "  const pickedPromise = new Promise((resolve) => {\n",
              "    inputElement.addEventListener('change', (e) => {\n",
              "      resolve(e.target.files);\n",
              "    });\n",
              "  });\n",
              "\n",
              "  const cancel = document.createElement('button');\n",
              "  inputElement.parentElement.appendChild(cancel);\n",
              "  cancel.textContent = 'Cancel upload';\n",
              "  const cancelPromise = new Promise((resolve) => {\n",
              "    cancel.onclick = () => {\n",
              "      resolve(null);\n",
              "    };\n",
              "  });\n",
              "\n",
              "  // Wait for the user to pick the files.\n",
              "  const files = yield {\n",
              "    promise: Promise.race([pickedPromise, cancelPromise]),\n",
              "    response: {\n",
              "      action: 'starting',\n",
              "    }\n",
              "  };\n",
              "\n",
              "  cancel.remove();\n",
              "\n",
              "  // Disable the input element since further picks are not allowed.\n",
              "  inputElement.disabled = true;\n",
              "\n",
              "  if (!files) {\n",
              "    return {\n",
              "      response: {\n",
              "        action: 'complete',\n",
              "      }\n",
              "    };\n",
              "  }\n",
              "\n",
              "  for (const file of files) {\n",
              "    const li = document.createElement('li');\n",
              "    li.append(span(file.name, {fontWeight: 'bold'}));\n",
              "    li.append(span(\n",
              "        `(${file.type || 'n/a'}) - ${file.size} bytes, ` +\n",
              "        `last modified: ${\n",
              "            file.lastModifiedDate ? file.lastModifiedDate.toLocaleDateString() :\n",
              "                                    'n/a'} - `));\n",
              "    const percent = span('0% done');\n",
              "    li.appendChild(percent);\n",
              "\n",
              "    outputElement.appendChild(li);\n",
              "\n",
              "    const fileDataPromise = new Promise((resolve) => {\n",
              "      const reader = new FileReader();\n",
              "      reader.onload = (e) => {\n",
              "        resolve(e.target.result);\n",
              "      };\n",
              "      reader.readAsArrayBuffer(file);\n",
              "    });\n",
              "    // Wait for the data to be ready.\n",
              "    let fileData = yield {\n",
              "      promise: fileDataPromise,\n",
              "      response: {\n",
              "        action: 'continue',\n",
              "      }\n",
              "    };\n",
              "\n",
              "    // Use a chunked sending to avoid message size limits. See b/62115660.\n",
              "    let position = 0;\n",
              "    do {\n",
              "      const length = Math.min(fileData.byteLength - position, MAX_PAYLOAD_SIZE);\n",
              "      const chunk = new Uint8Array(fileData, position, length);\n",
              "      position += length;\n",
              "\n",
              "      const base64 = btoa(String.fromCharCode.apply(null, chunk));\n",
              "      yield {\n",
              "        response: {\n",
              "          action: 'append',\n",
              "          file: file.name,\n",
              "          data: base64,\n",
              "        },\n",
              "      };\n",
              "\n",
              "      let percentDone = fileData.byteLength === 0 ?\n",
              "          100 :\n",
              "          Math.round((position / fileData.byteLength) * 100);\n",
              "      percent.textContent = `${percentDone}% done`;\n",
              "\n",
              "    } while (position < fileData.byteLength);\n",
              "  }\n",
              "\n",
              "  // All done.\n",
              "  yield {\n",
              "    response: {\n",
              "      action: 'complete',\n",
              "    }\n",
              "  };\n",
              "}\n",
              "\n",
              "scope.google = scope.google || {};\n",
              "scope.google.colab = scope.google.colab || {};\n",
              "scope.google.colab._files = {\n",
              "  _uploadFiles,\n",
              "  _uploadFilesContinue,\n",
              "};\n",
              "})(self);\n",
              "</script> "
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Saving case_studies_graph.json to case_studies_graph (1).json\n",
            "Using: case_studies_graph (1).json\n"
          ]
        }
      ],
      "source": [
        "from google.colab import files\n",
        "up = files.upload()\n",
        "json_path = next(iter(up.keys()))\n",
        "print(\"Using:\", json_path)"
      ],
      "id": "ulVbBiZxv2am"
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "njPfSJgbv2am"
      },
      "source": [
        "## 3) Load JSON and recreate `Node`/`Edge` objects"
      ],
      "id": "njPfSJgbv2am"
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "njCa4ZK-v2an"
      },
      "execution_count": null,
      "outputs": [],
      "source": [
        "import json\n",
        "from dataclasses import dataclass\n",
        "from typing import Any, Dict, List\n",
        "\n",
        "# Prefer the real classes; if import fails we fall back to simple shims.\n",
        "try:\n",
        "    from langchain_core.runnables.graph import Node as LCNode, Edge as LCEdge\n",
        "except Exception:\n",
        "    @dataclass\n",
        "    class LCNode:\n",
        "        id: str\n",
        "        name: str = \"\"\n",
        "        data: Dict[str, Any] = None\n",
        "        metadata: Dict[str, Any] = None\n",
        "    @dataclass\n",
        "    class LCEdge:\n",
        "        source: str\n",
        "        target: str\n",
        "        data: Dict[str, Any] = None\n",
        "        metadata: Dict[str, Any] = None\n",
        "\n",
        "def make_node_from_json(n: Dict[str, Any]):\n",
        "    raw = n.get(\"data\", {}) or {}\n",
        "    node_id = n[\"id\"]\n",
        "    # good human-readable name if available\n",
        "    name = raw.get(\"title\") or raw.get(\"name\") or node_id\n",
        "\n",
        "    # Try modern signature first: (id, name, data, metadata)\n",
        "    try:\n",
        "        return LCNode(id=node_id, name=name, data=raw, metadata={})\n",
        "    except TypeError:\n",
        "        pass\n",
        "    # Try legacy 2-arg versions\n",
        "    try:\n",
        "        return LCNode(id=node_id, data=raw)  # older langchain-core builds\n",
        "    except TypeError:\n",
        "        pass\n",
        "    try:\n",
        "        return LCNode(node_id, raw)  # positional\n",
        "    except TypeError:\n",
        "        pass\n",
        "    # Try alt shape: (name, metadata) etc.\n",
        "    try:\n",
        "        return LCNode(name=node_id, metadata=raw)\n",
        "    except TypeError:\n",
        "        pass\n",
        "    # Last resort: construct a dataclass-like shim\n",
        "    return LCNode(id=node_id, name=name, data=raw, metadata={})\n",
        "\n",
        "def make_edge_from_json(e: Dict[str, Any]):\n",
        "    raw = e.get(\"data\", {}) or {}\n",
        "    src, tgt = e[\"source\"], e[\"target\"]\n",
        "    # Modern: Edge(source, target, data, metadata?)\n",
        "    try:\n",
        "        return LCEdge(source=src, target=tgt, data=raw)\n",
        "    except TypeError:\n",
        "        pass\n",
        "    # Alt: metadata instead of data\n",
        "    try:\n",
        "        return LCEdge(source=src, target=tgt, metadata=raw)\n",
        "    except TypeError:\n",
        "        pass\n",
        "    # Positional\n",
        "    try:\n",
        "        return LCEdge(src, tgt, raw)\n",
        "    except TypeError:\n",
        "        pass\n",
        "    # Fallback shim\n",
        "    return LCEdge(source=src, target=tgt, data=raw, metadata={})\n",
        "\n",
        "# ---- Load your JSON graph\n",
        "with open(json_path, \"r\", encoding=\"utf-8\") as f:\n",
        "    obj = json.load(f)\n",
        "\n",
        "nodes: List[Any] = [make_node_from_json(n) for n in obj[\"nodes\"]]\n",
        "edges: List[Any] = [make_edge_from_json(e) for e in obj[\"edges\"]]\n",
        "\n",
        "len(nodes), len(edges)\n",
        "Node = LCNode\n",
        "Edge = LCEdge\n"
      ],
      "id": "njCa4ZK-v2an"
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "_YhTmZrgv2an"
      },
      "source": [
        "### Peek at a few nodes/edges"
      ],
      "id": "_YhTmZrgv2an"
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "w60RdZ8Jv2an",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "ac39af69-217f-42d3-f9e6-8a59adbcb55f"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "([Node(id='Client|Une entreprise de renommée mondiale, spécialisée dans la transformation de produits de la mer', name='Une entreprise de renommée mondiale, spécialisée dans la transformation de produits de la mer', data={'labels': ['Client'], 'name': 'Une entreprise de renommée mondiale, spécialisée dans la transformation de produits de la mer', 'Ownership_Type': None, 'Founding_Date': None, 'Number_of_Employees': None}, metadata={}),\n",
              "  Node(id=\"Project|Assurer le support au MCO sur l'ERP VIF en l'absence de l'équipe IT et des relais fonctionnels\", name=\"Assurer le support au MCO sur l'ERP VIF en l'absence de l'équipe IT et des relais fonctionnels\", data={'labels': ['Project'], 'title': \"Assurer le support au MCO sur l'ERP VIF en l'absence de l'équipe IT et des relais fonctionnels\", 'duration_months': 14, 'complexity': None, 'budget_estimate': 175000.0, 'time_to_close': 73, 'cost_k€': 84.0}, metadata={})],\n",
              " [Edge(source='Client|Une entreprise de renommée mondiale, spécialisée dans la transformation de produits de la mer', target=\"Project|Assurer le support au MCO sur l'ERP VIF en l'absence de l'équipe IT et des relais fonctionnels\", data={'type': 'COMMISSIONED'}, conditional=False),\n",
              "  Edge(source=\"Project|Assurer le support au MCO sur l'ERP VIF en l'absence de l'équipe IT et des relais fonctionnels\", target='Industry|Food Industry', data={'type': 'IN_INDUSTRY'}, conditional=False),\n",
              "  Edge(source=\"Project|Assurer le support au MCO sur l'ERP VIF en l'absence de l'équipe IT et des relais fonctionnels\", target='Location|France', data={'type': 'LOCATED_IN'}, conditional=False)])"
            ]
          },
          "metadata": {},
          "execution_count": 5
        }
      ],
      "source": [
        "\n",
        "nodes[:2], edges[:3]\n"
      ],
      "id": "w60RdZ8Jv2an"
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "wr6wXOQ6v2an"
      },
      "source": [
        "## 4) Build documents to index"
      ],
      "id": "wr6wXOQ6v2an"
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "03IuTX3Wv2an",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "ca5c040f-cc17-4cc0-dcba-b996c3f1f48f"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "(670,\n",
              " 1696,\n",
              " 'Node Client|Une entreprise de renommée mondiale, spécialisée dans la transformation de produits de la mer | name: Une entreprise de renommée mondiale, spécialisée dans la transformation de produits de')"
            ]
          },
          "metadata": {},
          "execution_count": 6
        }
      ],
      "source": [
        "\n",
        "from langchain_community.embeddings import HuggingFaceEmbeddings\n",
        "from langchain_chroma import Chroma\n",
        "from langchain.docstore.document import Document\n",
        "\n",
        "def node_to_text(n: Node) -> str:\n",
        "    lbls = n.data.get(\"labels\", [])\n",
        "    if isinstance(lbls, str):\n",
        "        lbls = [lbls]\n",
        "    # pick some common fields\n",
        "    parts = [f\"Node {n.id}\"]\n",
        "    if \"title\" in n.data:\n",
        "        parts.append(f\"title: {n.data['title']}\")\n",
        "    if \"name\" in n.data:\n",
        "        parts.append(f\"name: {n.data['name']}\")\n",
        "    for k, v in n.data.items():\n",
        "        if k in {\"labels\",\"title\",\"name\"}:\n",
        "            continue\n",
        "        if v is None or v == \"\":\n",
        "            continue\n",
        "        parts.append(f\"{k}: {v}\")\n",
        "    return \" | \".join(parts)\n",
        "\n",
        "def edge_to_text(e: Edge) -> str:\n",
        "    typ = e.data.get(\"type\", \"\")\n",
        "    attrs = {k:v for k,v in e.data.items() if k != \"type\" and v not in (None,\"\")}\n",
        "    return f\"Edge {e.source} -[{typ}]-> {e.target} | \" + \" \".join(f\"{k}:{v}\" for k,v in attrs.items())\n",
        "\n",
        "node_docs = [Document(page_content=node_to_text(n), metadata={\"kind\":\"node\",\"id\":n.id,\"labels\":n.data.get(\"labels\",[])}) for n in nodes]\n",
        "edge_docs = [Document(page_content=edge_to_text(e), metadata={\"kind\":\"edge\",\"source\":e.source,\"target\":e.target,\"type\":e.data.get(\"type\",\"\")}) for e in edges]\n",
        "\n",
        "len(node_docs), len(edge_docs), node_docs[0].page_content[:200]\n"
      ],
      "id": "03IuTX3Wv2an"
    },
    {
      "cell_type": "code",
      "source": [
        "# ✅ 1) Pin compatible packages, then RESTART runtime once (Runtime > Restart session)\n",
        "!pip -q install --upgrade --no-cache-dir \\\n",
        "  google-generativeai==0.8.5 \\\n",
        "  google-ai-generativelanguage==0.6.15 \\\n",
        "  langchain-google-genai>=1.0.6 \\\n",
        "  faiss-cpu\n",
        "\n"
      ],
      "metadata": {
        "id": "_GUxPVd2kO_q"
      },
      "id": "_GUxPVd2kO_q",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "olnoQJzmv2ao"
      },
      "source": [
        "## 5) Create a vector store (in-memory)"
      ],
      "id": "olnoQJzmv2ao"
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "sf8cf1XCv2ao"
      },
      "execution_count": null,
      "outputs": [],
      "source": [
        "import os, json\n",
        "from langchain_google_genai import GoogleGenerativeAIEmbeddings\n",
        "from langchain_community.vectorstores import FAISS\n",
        "from langchain.docstore.document import Document\n",
        "\n",
        "# ✅ 2) Provide your key explicitly to the embeddings class\n",
        "GOOGLE_API_KEY = \"\"  # <-- put your key here or use os.getenv(\"GOOGLE_API_KEY\")\n",
        "assert GOOGLE_API_KEY, \"Set GOOGLE_API_KEY (string).\"\n",
        "\n",
        "# Optional: make sure ADC isn’t accidentally used\n",
        "os.environ.pop(\"GOOGLE_APPLICATION_CREDENTIALS\", None)\n",
        "\n",
        "emb = GoogleGenerativeAIEmbeddings(\n",
        "    model=\"models/embedding-001\",\n",
        "    google_api_key=GOOGLE_API_KEY,\n",
        ")\n",
        "\n",
        "# ✅ 3) Keep metadata simple (no lists/dicts) for the vector store\n",
        "def _simple_meta(m):\n",
        "    out = {}\n",
        "    for k, v in (m or {}).items():\n",
        "        if isinstance(v, (str, int, float, bool)) or v is None:\n",
        "            out[k] = v\n",
        "        elif isinstance(v, (list, tuple)):\n",
        "            out[k] = \", \".join(map(str, v))\n",
        "        elif isinstance(v, dict):\n",
        "            out[k] = json.dumps(v, ensure_ascii=False)\n",
        "        else:\n",
        "            out[k] = str(v)\n",
        "    return out\n",
        "\n",
        "safe_node_docs = [Document(page_content=d.page_content, metadata=_simple_meta(d.metadata)) for d in node_docs]\n",
        "safe_edge_docs = [Document(page_content=d.page_content, metadata=_simple_meta(d.metadata)) for d in edge_docs]\n",
        "all_docs = safe_node_docs + safe_edge_docs\n",
        "\n",
        "# ✅ 4) Build FAISS and a retriever\n",
        "faiss_index = FAISS.from_documents(all_docs, emb)\n",
        "retriever = faiss_index.as_retriever(search_kwargs={\"k\": 8})\n",
        "\n",
        "# (optional) persist / reload\n",
        "faiss_index.save_local(\"faiss_graph\")\n",
        "# reload example:\n",
        "# faiss_index = FAISS.load_local(\"faiss_graph\", emb, allow_dangerous_deserialization=True)\n"
      ],
      "id": "sf8cf1XCv2ao"
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "n4lR_ItFv2ao"
      },
      "source": [
        "## 6) Build a retriever and helper for neighborhoods"
      ],
      "id": "n4lR_ItFv2ao"
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "8nlI-3Sjv2ao"
      },
      "execution_count": null,
      "outputs": [],
      "source": [
        "from collections import defaultdict\n",
        "\n",
        "# Use the FAISS index you built earlier\n",
        "retriever = faiss_index.as_retriever(search_kwargs={\"k\": 8})\n",
        "\n",
        "# --- Robust accessors (handle different langchain-core versions) ---\n",
        "def node_key(n):\n",
        "    # Prefer .id; fall back to .name\n",
        "    return getattr(n, \"id\", None) or getattr(n, \"name\", None)\n",
        "\n",
        "def edge_src(e):\n",
        "    return getattr(e, \"source\", None) or getattr(e, \"src\", None) or getattr(e, \"start\", None) or (getattr(e, \"data\", {}) or getattr(e, \"metadata\", {})).get(\"source\")\n",
        "\n",
        "def edge_tgt(e):\n",
        "    return getattr(e, \"target\", None) or getattr(e, \"dst\", None) or getattr(e, \"end\", None) or (getattr(e, \"data\", {}) or getattr(e, \"metadata\", {})).get(\"target\")\n",
        "\n",
        "# Index nodes by key\n",
        "by_id = {node_key(n): n for n in nodes if node_key(n) is not None}\n",
        "\n",
        "# Build adjacency\n",
        "out_edges = defaultdict(list)\n",
        "in_edges = defaultdict(list)\n",
        "for e in edges:\n",
        "    s, t = edge_src(e), edge_tgt(e)\n",
        "    if s is None or t is None:\n",
        "        continue\n",
        "    out_edges[s].append(e)\n",
        "    in_edges[t].append(e)\n",
        "\n",
        "def expand_neighborhood(hit_ids, hops=1):\n",
        "    seen_nodes = set(hit_ids)\n",
        "    frontier = set(hit_ids)\n",
        "    for _ in range(hops):\n",
        "        new_nodes = set()\n",
        "        for nid in list(frontier):\n",
        "            for e in out_edges.get(nid, []) + in_edges.get(nid, []):\n",
        "                s, t = edge_src(e), edge_tgt(e)\n",
        "                if s: new_nodes.add(s)\n",
        "                if t: new_nodes.add(t)\n",
        "        frontier = new_nodes - seen_nodes\n",
        "        seen_nodes |= new_nodes\n",
        "    sub_nodes = [by_id[i] for i in seen_nodes if i in by_id]\n",
        "    sub_edges = [e for e in edges if edge_src(e) in seen_nodes and edge_tgt(e) in seen_nodes]\n",
        "    return sub_nodes, sub_edges\n",
        "\n"
      ],
      "id": "8nlI-3Sjv2ao"
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "JoOTKRcUv2ao"
      },
      "source": [
        "## 7) Query examples"
      ],
      "id": "JoOTKRcUv2ao"
    },
    {
      "cell_type": "code",
      "source": [
        "os.environ[\"GOOGLE_API_KEY\"] = \"\""
      ],
      "metadata": {
        "id": "_J4tlcsIuuyI"
      },
      "id": "_J4tlcsIuuyI",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# === GraphRAG: generic Q&A over your graph with Gemini 2.0 Flash ===\n",
        "# Prereqs:\n",
        "# - `nodes`, `edges` lists exist (with .id/.data and .source/.target/.data)\n",
        "# - `faiss_index` exists (built from node_docs + edge_docs)\n",
        "# - set your key once: os.environ[\"GOOGLE_API_KEY\"] = \"...\"  (no hardcoding here!)\n",
        "\n",
        "import os, json\n",
        "from collections import defaultdict\n",
        "from typing import Any, Dict, List, Tuple\n",
        "from langchain_google_genai import ChatGoogleGenerativeAI\n",
        "from langchain_core.messages import SystemMessage, HumanMessage\n",
        "\n",
        "# --- minimal helpers (kept local) ---\n",
        "def _n_id(n): return getattr(n, \"id\", None) or getattr(n, \"name\", None) or \"\"\n",
        "def _n_data(n) -> Dict[str, Any]:\n",
        "    d = getattr(n, \"data\", None)\n",
        "    if not isinstance(d, dict) or d is None:\n",
        "        d = getattr(n, \"metadata\", {}) or {}\n",
        "    return d\n",
        "def _labels(n):\n",
        "    lbls = _n_data(n).get(\"labels\", [])\n",
        "    return [lbls] if isinstance(lbls, str) else (lbls or [])\n",
        "def _title(n):\n",
        "    d = _n_data(n)\n",
        "    return d.get(\"title\") or d.get(\"name\") or _n_id(n)\n",
        "\n",
        "def _e_data(e) -> Dict[str, Any]:\n",
        "    d = getattr(e, \"data\", None)\n",
        "    if not isinstance(d, dict) or d is None:\n",
        "        d = getattr(e, \"metadata\", {}) or {}\n",
        "    return d\n",
        "def _src(e): return getattr(e, \"source\", None) or _e_data(e).get(\"source\", \"\")\n",
        "def _tgt(e): return getattr(e, \"target\", None)  or _e_data(e).get(\"target\", \"\")\n",
        "\n",
        "# --- build adjacency once ---\n",
        "by_id = {_n_id(n): n for n in nodes}\n",
        "out_edges = defaultdict(list); in_edges = defaultdict(list)\n",
        "for e in edges:\n",
        "    s, t = _src(e), _tgt(e)\n",
        "    if s and t:\n",
        "        out_edges[s].append(e); in_edges[t].append(e)\n",
        "\n",
        "def expand_neighborhood(seed_ids, hops=1, max_nodes=1500):\n",
        "    seen = set(seed_ids); frontier = set(seed_ids)\n",
        "    for _ in range(hops):\n",
        "        new_nodes = set()\n",
        "        for nid in frontier:\n",
        "            for e in out_edges.get(nid, []) + in_edges.get(nid, []):\n",
        "                if _src(e): new_nodes.add(_src(e))\n",
        "                if _tgt(e): new_nodes.add(_tgt(e))\n",
        "        frontier = new_nodes - seen\n",
        "        seen |= new_nodes\n",
        "        if len(seen) >= max_nodes:\n",
        "            break\n",
        "    sub_nodes = [by_id[i] for i in seen if i in by_id]\n",
        "    sub_edges = [e for e in edges if _src(e) in seen and _tgt(e) in seen]\n",
        "    return sub_nodes, sub_edges\n",
        "\n",
        "def build_summary(ns: List[Any], es: List[Any], max_per_label=25, max_edges=400) -> str:\n",
        "    buckets = defaultdict(list)\n",
        "    for n in ns:\n",
        "        labs = _labels(n) or [\"Node\"]\n",
        "        for lab in labs:\n",
        "            buckets[lab].append(n)\n",
        "\n",
        "    lines = []\n",
        "    for lab, items in buckets.items():\n",
        "        for n in items[:max_per_label]:\n",
        "            d = _n_data(n)\n",
        "            brief = {\n",
        "                \"id\": _n_id(n),\n",
        "                \"title\": d.get(\"title\"),\n",
        "                \"name\": d.get(\"name\"),\n",
        "            }\n",
        "            # include a few salient numeric/text attrs if present\n",
        "            for k in (\"industry\",\"industries\",\"cluster_id\",\"type\",\"duration_months\",\"complexity\"):\n",
        "                if d.get(k) not in (None, \"\", []):\n",
        "                    brief[k] = d.get(k)\n",
        "            lines.append(f\"{lab}: \" + json.dumps({k:v for k,v in brief.items() if v is not None}, ensure_ascii=False))\n",
        "\n",
        "    for e in es[:max_edges]:\n",
        "        et = _e_data(e).get(\"type\",\"\")\n",
        "        lines.append(f\"EDGE: {_src(e)} -[{et}]-> {_tgt(e)}\")\n",
        "    return \"\\n\".join(lines)\n",
        "\n",
        "# --- build a generic GraphRAG answerer ---\n",
        "from langchain_community.vectorstores import FAISS  # for type clarity; already installed\n",
        "retriever = faiss_index.as_retriever(search_kwargs={\"k\": 12})  # bump k a bit\n",
        "\n",
        "def answer_question(\n",
        "    query: str,\n",
        "    hops: int = 1,\n",
        "    k: int = 12,\n",
        "    max_nodes: int = 1500,\n",
        "    model: str = \"gemini-2.0-flash\",\n",
        "    temperature: float = 0.0,\n",
        "):\n",
        "    # 1) retrieve\n",
        "    local_retriever = faiss_index.as_retriever(search_kwargs={\"k\": k})\n",
        "    docs = local_retriever.invoke(query)\n",
        "\n",
        "    # 2) seed IDs from node docs + edge docs\n",
        "    seed_ids = set()\n",
        "    for d in docs:\n",
        "        kind = d.metadata.get(\"kind\")\n",
        "        if kind == \"node\" and \"id\" in d.metadata:\n",
        "            seed_ids.add(d.metadata[\"id\"])\n",
        "        elif kind == \"edge\":\n",
        "            if \"source\" in d.metadata: seed_ids.add(d.metadata[\"source\"])\n",
        "            if \"target\" in d.metadata: seed_ids.add(d.metadata[\"target\"])\n",
        "\n",
        "    # fallback: parse edge strings if needed\n",
        "    if not seed_ids:\n",
        "        for d in docs:\n",
        "            if d.metadata.get(\"kind\") == \"edge\":\n",
        "                txt = d.page_content\n",
        "                try:\n",
        "                    s = txt.split(\"Edge \",1)[1].split(\" -[\",1)[0]\n",
        "                    t = txt.split(\"]-> \",1)[1].split(\" | \",1)[0]\n",
        "                    seed_ids.update([s,t])\n",
        "                except Exception:\n",
        "                    pass\n",
        "\n",
        "    # 3) expand to neighborhood\n",
        "    sub_nodes, sub_edges = expand_neighborhood(seed_ids, hops=hops, max_nodes=max_nodes)\n",
        "\n",
        "    # 4) compact to summary (token-friendly)\n",
        "    nodes_summary = build_summary(sub_nodes, sub_edges)\n",
        "\n",
        "    # 5) ask Gemini 2.0 Flash\n",
        "    api_key = os.getenv(\"GOOGLE_API_KEY\", \"\")\n",
        "    assert api_key, \"Please set env var GOOGLE_API_KEY.\"\n",
        "    llm = ChatGoogleGenerativeAI(model=model, google_api_key=api_key, temperature=temperature)\n",
        "\n",
        "    sys = SystemMessage(content=(\n",
        "        \"You are an expert strategic assistant to Talan. You are provided with a graph database of historical data \"\n",
        "        \"Clusters represent groups of similar challenges. to answer the question, reason over the content of the graph and perform any analysis necessary to uncover hidden patterns and assist decision making\"\n",
        "    ))\n",
        "    human = HumanMessage(content=f\"Subgraph:\\n{nodes_summary}\\n\\nQuestion:\\n{query}\")\n",
        "    resp = llm.invoke([sys, human])\n",
        "\n",
        "    # 6) pretty print + return raw bits if you want to reuse\n",
        "    print(f\"Retrieved docs: {len(docs)}  |  Seeds: {len(seed_ids)}  |  Subgraph: {len(sub_nodes)} nodes / {len(sub_edges)} edges\")\n",
        "    print(\"\\nAnswer:\\n\", getattr(resp, \"content\", resp))\n",
        "    return {\"answer\": getattr(resp, \"content\", resp), \"sub_nodes\": sub_nodes, \"sub_edges\": sub_edges, \"docs\": docs}\n",
        "\n",
        "# ---- Example usage (works for ANY question) ----\n",
        "# result = answer_question(\"Which projects in the Food Industry used SAP and what challenges were resolved?\", hops=1, k=12)\n",
        "result = answer_question(\"I have a new client who is suffering from aging sap system. what should i do\")\n",
        "#result = answer_question(\"Which departments correlate with high productivity scores across projects?\", hops=2)\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "1qCgXmTzuhyB",
        "outputId": "1aacda05-7d64-42ae-fb5f-c71224e0fdb4"
      },
      "id": "1qCgXmTzuhyB",
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Retrieved docs: 12  |  Seeds: 12  |  Subgraph: 51 nodes / 73 edges\n",
            "\n",
            "Answer:\n",
            " Based on the provided graph data, here's a strategic approach to assist your new client with their aging SAP systems:\n",
            "\n",
            "**1. Identify the Specific Challenges:**\n",
            "\n",
            "*   **Understand the Current State:**  The client is facing challenges related to aging SAP systems.  You need to pinpoint the *specific* issues.  Some common problems, as seen in the graph, include:\n",
            "    *   Obsolete SAP versions (ECC, BW, PI, CRM).\n",
            "    *   On-premise deployments that no longer meet business needs.\n",
            "    *   Aging hardware infrastructure.\n",
            "    *   Technical debt accumulation.\n",
            "    *   Lack of a centralized HR database.\n",
            "\n",
            "**2. Leverage Talan's Expertise and Solutions:**\n",
            "\n",
            "*   **SAP Modernization & Migration:** Talan has a strong track record in modernizing SAP environments.  The graph highlights several successful SAP S/4HANA migrations.\n",
            "*   **Cloud Solutions:**  Cloud migration is a recurring theme.  Talan has experience with:\n",
            "    *   Amazon Web Services (AWS)\n",
            "    *   Microsoft Azure\n",
            "    *   SAP S/4HANA Cloud (Private Edition)\n",
            "*   **Infrastructure Modernization:**  Talan has helped clients replace aging servers with cloud-based solutions.\n",
            "\n",
            "**3.  Recommended Actions:**\n",
            "\n",
            "*   **Assessment:** Conduct a thorough assessment of the client's existing SAP landscape.  This includes:\n",
            "    *   SAP version and infrastructure analysis.\n",
            "    *   Business requirements and future goals.\n",
            "    *   Identifying pain points and areas for improvement.\n",
            "*   **Solution Proposal:**  Based on the assessment, propose a tailored solution.  Consider these options:\n",
            "    *   **SAP S/4HANA Migration:**  If the client's business needs align, recommend a migration to SAP S/4HANA.  This could be on-premise, cloud-based, or a hybrid approach.\n",
            "    *   **Cloud Migration:**  If the client is open to the cloud, suggest migrating their SAP infrastructure to AWS or Azure.  This can address hardware obsolescence and improve scalability.\n",
            "    *   **Hybrid Approach:**  A hybrid approach might be suitable if the client has specific requirements for on-premise systems.\n",
            "*   **Project Planning:**  Develop a detailed project plan, including:\n",
            "    *   Timeline and milestones.\n",
            "    *   Resource allocation (including experienced Talan consultants).\n",
            "    *   Risk management strategy.\n",
            "    *   Change management plan (especially important for large user bases).\n",
            "\n",
            "**4.  Highlight Relevant Case Studies:**\n",
            "\n",
            "*   **Getlink:**  Modernized their SAP system with SAP S/4HANA in the cloud (Azure).  This is a good example if the client is looking for a comprehensive modernization.\n",
            "*   **Parrot:**  Migrated their SAP environment to AWS to overcome hardware obsolescence.  This is relevant if the client's primary concern is aging infrastructure.\n",
            "*   **Ecosystem:** Modernized SAP environments using cloud infrastructure.\n",
            "\n",
            "**5.  Team Composition:**\n",
            "\n",
            "*   The graph shows that senior employees with high productivity scores are often involved in these projects.  Ensure that the project team includes experienced SAP consultants with expertise in the chosen solution (e.g., SAP S/4HANA, cloud migration).\n",
            "\n",
            "By following this strategic approach, you can effectively address your new client's challenges and leverage Talan's expertise to deliver a successful SAP modernization project.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "BGd_XH8gv2ao"
      },
      "source": [
        "## 9) (Optional) Visualize a neighborhood"
      ],
      "id": "BGd_XH8gv2ao"
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "jiaHbHhvv2ap",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "aa4be04e-edb6-4205-dc0c-b896053723a6"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Warning: When  cdn_resources is 'local' jupyter notebook has issues displaying graphics on chrome/safari. Use cdn_resources='in_line' or cdn_resources='remote' if you have issues viewing graphics in a notebook.\n",
            "subgraph.html\n",
            "Wrote subgraph.html\n"
          ]
        }
      ],
      "source": [
        "\n",
        "import networkx as nx\n",
        "from pyvis.network import Network\n",
        "\n",
        "G = nx.Graph()\n",
        "for n in sub_nodes:\n",
        "    label = \",\".join(n.data.get(\"labels\", [])) or \"Node\"\n",
        "    title = n.data.get(\"title\") or n.data.get(\"name\") or n.id\n",
        "    G.add_node(n.id, label=label, title=title)\n",
        "for e in sub_edges:\n",
        "    et = e.data.get(\"type\",\"\")\n",
        "    G.add_edge(e.source, e.target, label=et)\n",
        "\n",
        "net = Network(notebook=True, height=\"500px\", width=\"100%\")\n",
        "net.from_nx(G)\n",
        "net.show(\"subgraph.html\")\n",
        "print(\"Wrote subgraph.html\")\n"
      ],
      "id": "jiaHbHhvv2ap"
    }
  ]
}